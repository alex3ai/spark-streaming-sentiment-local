services:
  # --- Apache Kafka Official (KRaft Mode) ---
  kafka:
    image: apache/kafka:3.7.0
    container_name: kafka
    hostname: kafka
    ports:
      - "9092:9092"
      - "9094:9094"
    volumes:
      - ./kafka_data:/var/lib/kafka/data
    environment:
      KAFKA_NODE_ID: 1
      KAFKA_PROCESS_ROLES: broker,controller
      KAFKA_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:9094
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://127.0.0.1:9094
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_LOG_RETENTION_HOURS: 168
    deploy:
      resources:
        limits:
          cpus: '0.50'
          memory: 768M

  # --- Spark Master ---
  spark-master:
    build:
      context: .
      dockerfile: infra/Dockerfile
    container_name: spark-master
    hostname: spark-master
    init: true
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.master.Master
    ports:
      - "9090:8080"
      - "7077:7077"
    volumes:
      - ./app:/app
      - ./data:/data
    environment:
      # SRE FIX: Networking. Sobrescreve o config.py para usar a rede interna do Docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      # Mem√≥ria ajustada para evitar OOM
      - SPARK_DAEMON_MEMORY=512m
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1536M

  # --- Spark Worker ---
  spark-worker:
    build:
      context: .
      dockerfile: infra/Dockerfile
    container_name: spark-worker
    hostname: spark-worker
    init: true
    command: /opt/spark/bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      - SPARK_MODE=worker
      - SPARK_WORKER_CORES=1
      # SRE FIX: Networking. Sobrescreve o config.py para usar a rede interna do Docker
      - KAFKA_BOOTSTRAP_SERVERS=kafka:9092
      - SPARK_WORKER_MEMORY=512m
    volumes:
      - ./app:/app
      - ./data:/data
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1536M
